{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                   Sama\n",
      "1     Global Elite Texas\n",
      "2              Risk Labs\n",
      "3               Tractian\n",
      "4              Livefront\n",
      "5              Deep 6 AI\n",
      "6               Codility\n",
      "7         Delphi Digital\n",
      "8                    1kx\n",
      "9               EoT Labs\n",
      "10                 Eleks\n",
      "11               DFINITY\n",
      "12         Manta Network\n",
      "13          Horizen Labs\n",
      "14                 Aztec\n",
      "15                 STEPN\n",
      "16              OfferFit\n",
      "17              BenchSci\n",
      "18             Risk Labs\n",
      "19               Apptegy\n",
      "20                Gopuff\n",
      "21                 LI.FI\n",
      "22                  Enya\n",
      "23             Waterfall\n",
      "24            MentorMate\n",
      "25              Pangolia\n",
      "26                 Canva\n",
      "27              Skydance\n",
      "28              Skydance\n",
      "Name: company, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import xlwt\n",
    "from xlwt import Workbook\n",
    "import smtplib\n",
    "from os.path import basename\n",
    "from email.mime.application import MIMEApplication\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "import email.utils\n",
    "import pandas as pd\n",
    "\n",
    "api_url='https://remoteok.com/api/'\n",
    "user_agent='Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'\n",
    "REQUEST_HEADER={\n",
    "    'User-Agent':user_agent,\n",
    "    'Accept-Language':'en-US,en;q=0.5',\n",
    "\n",
    "}\n",
    "\n",
    "def get_job_posting():\n",
    "    res=requests.get(api_url,headers=REQUEST_HEADER)\n",
    "    return res.json()\n",
    "\n",
    "def output_job_to_excel(data):\n",
    "    wb=Workbook()\n",
    "    job_sheet=wb.add_sheet(\"Jobs\")\n",
    "    header=list(data[0].keys())\n",
    "    for i in range(len(header)):\n",
    "        job_sheet.write(0,i,header[i])\n",
    "    # wb.save('remoteok_data.xls')\n",
    "    for i in range(len(data)):\n",
    "        dataRow=data[i]\n",
    "        values=list(dataRow.values())\n",
    "        for j in range(len(values)):\n",
    "            job_sheet.write(i+1,j,values[j])\n",
    "    wb.save('remoteok_data.xls')\n",
    "    \n",
    "def scrap(start,end):\n",
    "    json=get_job_posting()[int(start):int(end)]\n",
    "    output_job_to_excel(json)\n",
    "    print(pd.DataFrame(json)['company'])\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    scrap(1,30)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import requests\n",
    "import csv\n",
    "import bs4\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "USER_AGENT='Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'\n",
    "REQUEST_HEADER={\n",
    "    'User-Agent':USER_AGENT,\n",
    "    'Accept-Language':'en-US,en;q=0.5',\n",
    "\n",
    "}\n",
    "NO_THREADS=5\n",
    "def get_page_html(url):\n",
    "    res=requests.get(url=url,headers=REQUEST_HEADER)\n",
    "    return res.content\n",
    "def get_product_price(soup):\n",
    "    main_price_span=soup.find('span',attrs={\n",
    "        'class':'a-price-whole'\n",
    "    })\n",
    "    \n",
    "    return main_price_span.get_text(strip=True).strip().replace('.','')\n",
    "def get_product_title(soup):\n",
    "    product_title=soup.find('span',id='productTitle')\n",
    "    return product_title.text.strip()\n",
    "\n",
    "def get_product_rating(soup):\n",
    "    product_rating_div=soup.find('div',attrs={\n",
    "        'id':'averageCustomerReviews'\n",
    "    })\n",
    "    product_rating_section=product_rating_div.find('span', id='acrPopover',attrs={\n",
    "        'class':\"reviewCountTextLinkedHistogram noUnderline\"\n",
    "    })\n",
    "\n",
    "    product_rating=product_rating_section.find('span',attrs={\n",
    "        'class':'a-icon-alt'\n",
    "    })\n",
    "    return float(product_rating.get_text(strip=True).strip().split()[0])\n",
    "\n",
    "\n",
    "def extract_product_info(url,output):\n",
    "    product_info={}\n",
    "    html=get_page_html(url)\n",
    "    soup=bs4.BeautifulSoup(html,'lxml')\n",
    "\n",
    "    product_info['title']=get_product_title(soup)\n",
    "    product_info['price']=get_product_price(soup)\n",
    "    product_info['rating']=get_product_rating(soup)\n",
    "    output.append( product_info)\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    product_data=[]\n",
    "    urls=[]\n",
    "    with open(\"amazon_web.csv\",newline='')as csvFile:\n",
    "        urls=list(csv.reader(csvFile,delimiter=','))\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=NO_THREADS) as executor:\n",
    "        for wkn in tqdm(range(0,len(urls))):\n",
    "            executor.submit(extract_product_info,urls[wkn][0],product_data)\n",
    "    output_file='amazon_product_data-{}.csv'.format(\n",
    "        datetime.today().strftime(\"%d_%m_%Y\")\n",
    "    )\n",
    "    with open(output_file,'w',newline='')as outputFile:\n",
    "        write=csv.writer(outputFile)\n",
    "        write.writerow(product_data[0].keys())\n",
    "        for product in product_data:\n",
    "            write.writerow(product.values())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
